# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lUfzb7cLAeD4uOfCyRumtUqkP3Ya9Uv2
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import re
import tensorflow as tf
from collections import Counter

train_df = pd.read_csv("/content/train.csv")
test_df = pd.read_csv("/content/test.csv")

train_df.head()

test_df.head()

#Number of Missing Data For Train Data
keyword_null = train_df['keyword'].isna().sum()
print("Keyword Null Count : " , keyword_null)
location_null = train_df['location'].isna().sum()
print("Location Null Count : " , location_null)

#Train Data Keyword Nulls Percentage
print("Keyword Null Percentage Value : " , (keyword_null/(train_df.shape[0])*100))

#Train Data Location Nulls Percentage
print("Location Null Percentage value : " , (location_null/(train_df.shape[0])*100))

#Number of Missing Data For Test Data
test_keyword_null = test_df['keyword'].isna().sum()
print("Keyword Null Count : " , test_keyword_null)
test_location_null = train_df['location'].isna().sum()
print("Location Null Count : " , test_location_null)

#Test Data Keyword Nulls Percentage
print("Test Keyword Null Percentage value : " , (test_keyword_null/(test_df.shape[0])*100))

#Test Data Location Nulls Percentage
print("Test Location Null Percentage value : " , (test_location_null/(train_df.shape[0])*100))

#How many data in each class in train data
train_df['target'].value_counts().plot(kind ='pie', autopct='%.0f%%', figsize = (15,12))

#Top 15 locations in Train Data
train_df['location'].value_counts()[:15].index.tolist()

#Top 15 keywords in Train Data
train_df['keyword'].value_counts()[:15].index.tolist()

#Top 15 locations in Test Data
test_df['keyword'].value_counts()[:15].index.tolist()

#Top 15 keyword in Test Data
test_df['keyword'].value_counts()[:15].index.tolist()

#Most Common Words
pd.Series(' '.join(train_df['text']).lower().split()).value_counts()[:10]

pos_words = []
neg_words = []
for text in train_df[train_df['target']==1].text.str.split():
  for subtext in text:
    pos_words.append(subtext)
for text in train_df[train_df['target']==0].text.str.split():
  for subtext in text:
    neg_words.append(subtext)

all_words = pos_words + neg_words

most_common_words = Counter(all_words)

most_common_words = most_common_words.most_common()[:10]

ind = []
fre = []
for item in most_common_words:
   ind.append(item[0])
   fre.append(item[1])
fig = plt.figure(figsize= (15,10))

plt.bar(ind, fre)
plt.ylabel('Count')
plt.xlabel('Words')
plt.title("Most Common Words")
plt.show()

#Removing stopwords
import nltk.corpus
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_word = stopwords.words('english')
no_stop_words = []
stop_words = []
for word in all_words:
  if word not in stop_word:
    no_stop_words.append(word)
  else:
    stop_words.append(word)

final_stopwords = Counter(stop_words)

most_common_stopwords = final_stopwords.most_common()[:10]

most_common_stopwords

#Bar Plot for Most Common Words
ind = []
fre = []
for item in most_common_stopwords:
   ind.append(item[0])
   fre.append(item[1])
fig = plt.figure(figsize= (15,10))

plt.bar(ind, fre)
plt.ylabel('Count')
plt.xlabel('StopWords')
plt.title("Most Common Stopwords")
plt.show()

"""#NLP"""

#Cleaning the tweet
def clean_tweet(tweet):
  tweet = re.sub(r'@\w*','',tweet) 
  tweet = re.sub(r'\W+',' ',tweet) 
  tweet = re.sub(r'https?:\S*','',tweet)
  tweet = re.sub(r'\d*','',tweet)
  

  return tweet

clean_tweets = []
text_array = np.array(train_df['text'])
for text in text_array:
  clean_tweets.append(clean_tweet(text))

#Split Text
splitted_tweet = []
for text in clean_tweets:
  splitted_tweet.append(text.split())

#Remove Stopwords after removing regular expressions
stop_words = stopwords.words('english')
re_no_stop_words = []
re_no_stop_words_tweet = []
re_stopwords = []
for tweet in splitted_tweet:
  re_no_stop_words = []
  for word in tweet:
    if word not in stop_words:
      re_no_stop_words.append(word)
    else:
      re_stopwords.append(word)
  
  re_no_stop_words_tweet.append(list(re_no_stop_words))

#Keeping in the tweet only verbs, pronouns, nouns and adjectives
re_no_stop_words_tweets = []
re_no_stop_words_tweets_f = []
for i in re_no_stop_words_tweet:
  re_no_stop_words_tweets = []
  tweet = nltk.pos_tag(i)
  for j in tweet:
    if j[1][0] in ['N','R','V', 'J']:
      re_no_stop_words_tweets.append(j[0])
  re_no_stop_words_tweets_f.append(list(re_no_stop_words_tweets))
re_no_stop_words_tweets_f[0]

re_no_stop_words_tweets_f[0]

#Stemwords for data
from nltk.stem.snowball import SnowballStemmer
snow_stemmer = SnowballStemmer(language='english')
stem_words = []
stemwords = []
for i in re_no_stop_words_tweets_f:
  stem_words = []
  for j in i:
    x = snow_stemmer.stem(j)
    stem_words.append(x)
  stemwords.append(list(stem_words))
stemwords[0]

#Tokenization
from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer=Tokenizer()
tokenizer.fit_on_texts(stemwords)
tokenized_tweet =tokenizer.texts_to_sequences(stemwords)
# every word get a value which represent the importance of this word

#Padding
from tensorflow.keras.preprocessing.sequence import pad_sequences
padded=pad_sequences(tokenized_tweet)

padded[:10]

target = train_df['target'].values
target

#Splitting
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded, target, test_size=0.2, random_state=42)

#LSTM Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense,Dropout,Embedding,Bidirectional

model = Sequential()

model.add(Embedding(len(tokenizer.word_index)+1,10,input_length=26))
model.add(Bidirectional(LSTM(5)))
model.add(Dense(32,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32,activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(1,activation='sigmoid'))

model.compile(loss="binary_crossentropy",optimizer='RMSprop',metrics=['accuracy'])
model.summary()

model.compile(loss = "binary_crossentropy", optimizer = "RMSprop", metrics = ["accuracy"])

history = model.fit(X_train, y_train, validation_split = 0.1, verbose=1, epochs = 10, batch_size = 600)

model.evaluate(X_test, y_test)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['Train Accuracy','Val Accuracy'])
plt.title('Accuracy')
plt.xlabel('Epochs')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['training_Loss','Val_Loss'])
plt.title('Loss')
plt.xlabel('epochs')

model.save("Disaster_tweet_model.h5")

